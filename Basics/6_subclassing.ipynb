{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Subclassing with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Configure GPU memory growth to be dynamic instead of allocating all memory at once\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0\n",
    "x_test = x_test.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Definition\n",
    "\n",
    "So far we have noticed a recurring structure which looks like:\n",
    "CNN -> BatchNorm -> ReLU (common structure)\n",
    "if we had to do it x10 times (lot of code to write!)\n",
    "\n",
    "- `layers.Layer` will keep track of everything under the hood for doing backpropagation and gradient descent, etc. Layers are the basic building blocks of neural networks in Keras. A layer encapsulates both a state (the layer's \"weights\") and a transformation from inputs to outputs (a **call**, the layer's forward pass).\n",
    "\n",
    "- Keras subclassing is exactly similar to creating **PyTorch** models. We can define the layers in the `__init__` method and the forward pass in the `call` method.\n",
    "\n",
    "- In the `__init__` we use `super` to call the `__init__` of the parent class (in this case `layers.Layer`).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBlock(layers.Layer):\n",
    "    def __init__(self, out_channels, kernel_size=3):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        # Define layers here\n",
    "        self.conv = layers.Conv2D(out_channels, kernel_size, padding=\"same\")\n",
    "        self.bn = layers.BatchNormalization()\n",
    "\n",
    "    \"\"\"\n",
    "    call() method is used to define the computation that should be performed on the\n",
    "    input given to the layer object. \n",
    "\n",
    "    :param input_tensor: input tensor\n",
    "    :param training: argument is used to differentiate between training and inference\n",
    "    becuase some layers have different behavior during training and inference (e.g. Dropout, BatchNorm)\n",
    "    When we do model.fit() that will set training=True, and when we do \n",
    "    model.evaluate() or model.predict() that will set training=False\n",
    "    \"\"\"    \n",
    "    def call(self, input_tensor, training=False):\n",
    "        x = self.conv(input_tensor)\n",
    "        # You can also print the shape here, etc. for debugging\n",
    "        # print(x.shape)\n",
    "        x = self.bn(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x\n",
    "\n",
    "# model = keras.Sequential(\n",
    "#     [\n",
    "#         CNNBlock(32), \n",
    "#         CNNBlock(64), \n",
    "#         CNNBlock(128), \n",
    "#         layers.Flatten(), \n",
    "#         layers.Dense(10),\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Complex Models\n",
    "\n",
    "- You should you `keras.Model` in your final model. `keras.Model` is a subclass of `keras.layers.Layer` and it has all the functionality of `keras.layers.Layer` and more.\n",
    "\n",
    "- For the final model we use `keras.Model` instead of `keras.layers.Layer`. This is because we want to use the `fit` method of `keras.Model` which is not available in `keras.layers.Layer`. You can also do `model.layers` to get the list of layers in the model. `model.summary()` will give you a summary of the model. You can also do serialization and deserialization of the model using `model.save()` and `keras.models.load_model()`, save your model as a `h5` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(layers.Layer):\n",
    "    def __init__(self, channels):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.cnn1 = CNNBlock(channels[0], 3)\n",
    "        self.cnn2 = CNNBlock(channels[1], 3)\n",
    "        self.cnn3 = CNNBlock(channels[2], 3)\n",
    "        self.pooling = layers.MaxPooling2D()\n",
    "\n",
    "        # We are using the same cnn three times, so the height and the width of the output\n",
    "        # we get from these wont change but as each cnn layer has different number of channels,\n",
    "        # they might not be the same. Hence we do identity mapping to ensure that we have same\n",
    "        # number of channels. We do this by using kernel size = 1, which will change the\n",
    "        # number of channels as output.\n",
    "        self.identity_mapping = layers.Conv2D(channels[1], kernel_size = 1, padding=\"same\")\n",
    "\n",
    "    def call(self, input_tensor, training=False):\n",
    "        x = self.cnn1(input_tensor, training=training)\n",
    "        x = self.cnn2(x, training=training)\n",
    "        x = self.cnn3(x + self.identity_mapping(input_tensor), training=training,)\n",
    "        x = self.pooling(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResNet_Like(keras.Model):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet_Like, self).__init__()\n",
    "        self.block1 = ResBlock([32, 32, 64])\n",
    "        self.block2 = ResBlock([128, 128, 256])\n",
    "        self.block3 = ResBlock([128, 256, 512])\n",
    "        # Average pool the height and the width\n",
    "        # you can also do layer.Flatten() \n",
    "        self.pool = layers.GlobalAveragePooling2D()\n",
    "        self.classifier = layers.Dense(num_classes)\n",
    "\n",
    "    def call(self, input_tensor, training=False):\n",
    "        x = self.block1(input_tensor, training=training)\n",
    "        x = self.block2(x, training=training)\n",
    "        x = self.block3(x, training=training)\n",
    "        x = self.pool(x, training=training)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    # This is used to print the summary of the model\n",
    "    def model(self):\n",
    "        x = keras.Input(shape=(28, 28, 1))\n",
    "        return keras.Model(inputs=[x], outputs=self.call(x))\n",
    "\n",
    "\n",
    "model = ResNet_Like().model()\n",
    "base_input = model.layers[0].input\n",
    "base_output = model.layers[2].output\n",
    "output = layers.Dense(10)(layers.Flatten()(base_output))\n",
    "model = keras.Model(base_input, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "938/938 - 43s - loss: 0.1034 - accuracy: 0.9687\n",
      "Evaluating model...\n",
      "157/157 - 7s - loss: 0.0328 - accuracy: 0.9896\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.03283665329217911, 0.9896000027656555]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Training model...\")\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=1, verbose=2)\n",
    "\n",
    "print(\"Evaluating model...\")\n",
    "model.evaluate(x_test, y_test, batch_size=64, verbose=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
