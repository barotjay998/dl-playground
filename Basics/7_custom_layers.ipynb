{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-07 08:21:37.664273: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2024-01-07 08:21:37.665271: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2024-01-07 08:21:37.672039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-07 08:21:37.672247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1660 Ti computeCapability: 7.5\n",
      "coreClock: 1.59GHz coreCount: 24 deviceMemorySize: 5.80GiB deviceMemoryBandwidth: 268.26GiB/s\n",
      "2024-01-07 08:21:37.672264: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2024-01-07 08:21:37.673926: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2024-01-07 08:21:37.673978: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2024-01-07 08:21:37.675717: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2024-01-07 08:21:37.676070: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2024-01-07 08:21:37.678257: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2024-01-07 08:21:37.679397: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2024-01-07 08:21:37.682863: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2024-01-07 08:21:37.683000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-07 08:21:37.683361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-07 08:21:37.683486: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Configure GPU memory growth to be dynamic instead of allocating all memory at once\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(-1, 28 * 28).astype(\"float32\") / 255.0\n",
    "x_test = x_test.reshape(-1, 28 * 28).astype(\"float32\") / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Definition\n",
    "\n",
    "Most of the time it is fine to use the layers provided by Keras but sometimes you need to implement your own layer, and to understandd what exactly is happening under the hood. This can be done by subclassing the `Layer` class and implementing some methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model WITHOUT custom layers, uses the built-in Keras Dense and ReLU layers.\n",
    "\"\"\"\n",
    "class MyModel(keras.Model):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        # We want to create our own Dense layer and not\n",
    "        # use the one provided by Keras directly.\n",
    "        self.dense1 = layers.Dense(64)\n",
    "        self.dense2 = layers.Dense(num_classes)\n",
    "\n",
    "        # self.dense1 = layers.Dense(64)\n",
    "        # self.dense3 = layers.Dense(num_classes)\n",
    "\n",
    "    def call(self, input_tensor):\n",
    "        # We want to create our own ReLU layer and not\n",
    "        # use the one provided by Keras directly.\n",
    "        x = tf.nn.relu(self.dense1(input_tensor))\n",
    "        return self.dense2(x)\n",
    "\n",
    "\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Special methods:\n",
    "    - `__init__()`: the constructor of the class, it takes the following arguments:\n",
    "        - `name`: the name of the layer, **important to save and load models.**\n",
    "        - `trainable`: whether the layer should be trained or not\n",
    "        - `dtype`: the data type of the layer\n",
    "        - `dynamic`: whether the layer is dynamic or not\n",
    "        - `**kwargs`: other arguments\n",
    "    - `build()`: will build the layer, it takes the following arguments:\n",
    "        - `input_shape`: the shape of the input to the layer\n",
    "    - `call()`: will call the layer, it takes the following arguments:\n",
    "        - `inputs`: the input to the layer\n",
    "        - `training`: whether the layer is in training mode or not\n",
    "    - `compute_output_shape()`: will compute the output shape of the layer, it takes the following arguments:\n",
    "        - `input_shape`: the shape of the input to the layer\n",
    "    - `get_config()`: will get the configuration of the layer, it takes no arguments\n",
    "    - `from_config()`: will create a layer from the configuration of the layer, it takes the following arguments:\n",
    "        - `config`: the configuration of the layer\n",
    "    - `get_weights()`: will get the weights of the layer, it takes no arguments\n",
    "    - `set_weights()`: will set the weights of the layer, it takes the following arguments:\n",
    "        - `weights`: the weights of the layer\n",
    "    - `get_updates()`: will get the updates of the layer, it takes the following arguments:\n",
    "        - `inputs`: the input to the layer\n",
    "    - `add_update()`: will add an update to the layer, it takes the following arguments:\n",
    "        - `updates`: the update to be added to the layer\n",
    "        - `inputs`: the input to the layer\n",
    "    - `add_loss()`: will add a loss to the layer, it takes the following arguments:\n",
    "        - `losses`: the loss to be added to the layer\n",
    "        - `inputs`: the input to the layer\n",
    "    - `losses`: the losses of the layer\n",
    "    - `built`: whether the layer is built or not\n",
    "    - `supports_masking`: whether the layer supports masking or not\n",
    "    - `compute_mask()`: will compute the mask of the layer, it takes the following arguments:\n",
    "        - `inputs\n",
    "\n",
    "\n",
    "- `self.add_weight()`: will add a weight variable to the layer. It is a method of the `Layer` class. It takes the following arguments:\n",
    "    - `name`: the name of the weight variable, **important to save and load models.**\n",
    "    - `shape`: the shape of the weight variable\n",
    "    - `dtype`: the data type of the weight variable\n",
    "    - `initializer`: the initializer to be used to initialize the weight variable, e.g. random_normal, random_uniform, etc.\n",
    "    - `regularizer`: the regularizer to be used to regularize the weight variable\n",
    "    - `trainable`: whether the weight variable should be trained or not\n",
    "    - `constraint`: the constraint to be used to constrain the weight variable\n",
    "\n",
    "- There are other ways to add weight as well, you can use tf.variable but this is the simplest way to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model WITH custom layers, uses the custom ReLU and Dense layers.\n",
    "\"\"\"\n",
    "class Dense(layers.Layer):\n",
    "    def __init__(self, units, input_dim):\n",
    "        super(Dense, self).__init__()\n",
    "        self.w = self.add_weight(\n",
    "            name=\"w\",\n",
    "            shape=(input_dim, units),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "        # b is the bias term (offset).\n",
    "        self.b = self.add_weight(\n",
    "            name=\"b\", shape=(units,), initializer=\"zeros\", trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "\n",
    "# Dense with build method to avoid specifying input_dim\n",
    "class Dense2(layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(Dense, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(\n",
    "            name=\"w\",\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            name=\"b\", shape=(self.units,), initializer=\"random_normal\", trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "\n",
    "class MyReLU(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(MyReLU, self).__init__()\n",
    "\n",
    "    def call(self, x):\n",
    "        return tf.math.maximum(x, 0)\n",
    "\n",
    "\n",
    "class MyModel(keras.Model):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        # Instiantiate classes here\n",
    "\n",
    "        # self.dense1 = Dense(64, input_dim=784)\n",
    "        # self.dense2 = Dense(num_classes, input_dim=64)\n",
    "        \n",
    "        # As you can see, in layers.Dense you did not have to\n",
    "        # even specify input_dim. This is because this layer\n",
    "        # is tightly integrated with the rest of the framework.\n",
    "        # How do we achieve the same thing here? for our custom\n",
    "        # Dense layer.\n",
    "        # self.dense1 = layers.Dense(64)\n",
    "        # self.dense2 = layers.Dense(num_classes)\n",
    "\n",
    "        self.dense1 = Dense2(64)\n",
    "        self.dense2 = Dense2(num_classes)\n",
    "        self.relu = MyReLU()\n",
    "\n",
    "    def call(self, input_tensor):\n",
    "        # We want to create our own ReLU layer and not\n",
    "        # use the one provided by Keras directly.\n",
    "\n",
    "        # x = tf.nn.relu(self.dense1(input_tensor))\n",
    "\n",
    "        x = self.relu(self.dense1(input_tensor))\n",
    "        return self.dense2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1875/1875 - 1s - loss: 0.3007 - accuracy: 0.9160\n",
      "Epoch 2/2\n",
      "1875/1875 - 1s - loss: 0.1474 - accuracy: 0.9571\n",
      "313/313 - 0s - loss: 0.1223 - accuracy: 0.9640\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.12228093296289444, 0.9639999866485596]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=32, epochs=2, verbose=2)\n",
    "model.evaluate(x_test, y_test, batch_size=32, verbose=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
