{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN, GRU, LSTM and Bidirectionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Configure GPU memory growth to be dynamic instead of allocating all memory at once\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test  = x_test.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Definition\n",
    "\n",
    "### 3.1.1 RNN\n",
    "- `model.add(keras.Input(shape=(None, 28)))` we specify None because <ins>we dont have to have specific number of time steps</ins>. We have 28 pixels in one row of the image. But we put None as we dont have to specify that dimension.\n",
    "\n",
    "- `return_sequences=True` will return the output of the RNN layer for each time step. If we dont specify this, it will return the output of the RNN layer for the last time step. We do this so that we can stack multiple RNN layers. So in the code below the output from the RNN are 512 nodes and return_sequences=True will return 512 for each time step, in this case 28 time steps.\n",
    "    - In the `model.summary()` of this RNN you will notice output shape (None, None, 512) One for batch size, one for hidden states (time steps) and 512 are the nodes in the hidden state.\n",
    "\n",
    "- Notice in the code that for the second RNN we dont do return_sequences=True. This is because we only want the output of the last time step. Which is passed to the Dense layer which has 10 nodes for the 10 classes.\n",
    "    - In the `model.summary()` of this RNN you will notice output shape (None, 512) One for batch size and 512 from the last hidden state (time step) of the RNN layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn (SimpleRNN)       (None, None, 512)         276992    \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 806,922\n",
      "Trainable params: 806,922\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.Input(shape=(None, 28)))\n",
    "model.add(layers.SimpleRNN(512, return_sequences=True, activation=\"relu\"))\n",
    "model.add(layers.SimpleRNN(512, activation=\"relu\"))\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 RNN tanh\n",
    "- The default activation function for RNN is **tanh**. We can change it to relu or sigmoid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_2 (SimpleRNN)     (None, None, 256)         72960     \n",
      "_________________________________________________________________\n",
      "simple_rnn_3 (SimpleRNN)     (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 206,858\n",
      "Trainable params: 206,858\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.Input(shape=(None, 28)))\n",
    "model.add(layers.SimpleRNN(256, return_sequences=True, activation=\"tanh\"))\n",
    "model.add(layers.SimpleRNN(256))\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.keras.Sequential()\n",
    "model.add(keras.Input(shape=(None, 28)))\n",
    "model.add(layers.GRU(256, return_sequences=True, activation=\"tanh\"))\n",
    "model.add(layers.GRU(256))\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.keras.Sequential()\n",
    "model.add(keras.Input(shape=(None, 28)))\n",
    "model.add(layers.LSTM(256, return_sequences=True, activation=\"tanh\"))\n",
    "model.add(layers.LSTM(256))\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 Single Bidirectional LSTM\n",
    "\n",
    "In the `model.summary()` of this Bidirectional LSTM you will notice that the output shape doubled from 256 to 512. This is because the output of the Bidirectional LSTM is the concatenation of the forward LSTM and the backward LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional (Bidirectional (None, None, 512)         583680    \n",
      "_________________________________________________________________\n",
      "lstm_layer2 (LSTM)           (None, 256)               787456    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 1,373,706\n",
      "Trainable params: 1,373,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.Input(shape=(None, 28)))\n",
    "model.add(\n",
    "    layers.Bidirectional(layers.LSTM(256, return_sequences=True, activation=\"relu\"))\n",
    ")\n",
    "model.add(layers.LSTM(256, name=\"lstm_layer2\"))\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 Stacked Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.Input(shape=(None, 28)))\n",
    "model.add(\n",
    "    layers.Bidirectional(layers.LSTM(256, return_sequences=True, activation=\"relu\"))\n",
    ")\n",
    "model.add(\n",
    "    layers.Bidirectional(layers.LSTM(256, name=\"lstm_layer2\"))\n",
    "    )\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.Adam(lr=0.001),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Epoch 1/10\n",
      "938/938 - 18s - loss: 0.2994 - accuracy: 0.9089\n",
      "Epoch 2/10\n",
      "938/938 - 20s - loss: 0.1846 - accuracy: 0.9466\n",
      "Epoch 3/10\n",
      "938/938 - 23s - loss: 0.1593 - accuracy: 0.9550\n",
      "Epoch 4/10\n",
      "938/938 - 23s - loss: 0.1478 - accuracy: 0.9570\n",
      "Epoch 5/10\n",
      "938/938 - 24s - loss: 0.1447 - accuracy: 0.9579\n",
      "Epoch 6/10\n",
      "938/938 - 25s - loss: 0.1403 - accuracy: 0.9598\n",
      "Epoch 7/10\n",
      "938/938 - 21s - loss: 0.1414 - accuracy: 0.9588\n",
      "Epoch 8/10\n",
      "938/938 - 19s - loss: 0.1323 - accuracy: 0.9617\n",
      "Epoch 9/10\n",
      "938/938 - 19s - loss: 0.1374 - accuracy: 0.9603\n",
      "Epoch 10/10\n",
      "938/938 - 19s - loss: 0.1702 - accuracy: 0.9498\n",
      "\n",
      "Evaluating model...\n",
      "Test loss: 0.1770\n",
      "Test accuracy: 0.9464\n"
     ]
    }
   ],
   "source": [
    "print(\"Training model...\")\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=10, verbose=2)\n",
    "\n",
    "print(\"\\nEvaluating model...\")\n",
    "results = model.evaluate(x_test, y_test, batch_size=64, verbose=0)\n",
    "print(f\"Test loss: {results[0]:.4f}\")\n",
    "print(f\"Test accuracy: {results[1]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
