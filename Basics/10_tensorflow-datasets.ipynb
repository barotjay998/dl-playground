{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Datasets\n",
    "\n",
    "`tf.data` is a hihg-level API for reading data and transforming it into a form used for training. It is designed to work with TensorFlow and makes it easy to load data, manipulate it, and pipe it into a model. In short, It is a low-level API that can be used to build data pipelines. It should not be mistaken something to be used for loading only tensorflow datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Configure GPU memory growth to be dynamic instead of allocating all memory at once\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Datapipeline for Image Classification\n",
    "\n",
    "- `suffle_files=True` tensorflow usually store data in tf records, which is usually stored in multiple files. This option shuffles the files before loading them. For example for cifar10, there are 6 files, each containing 10,000 images. This seggregation is done to make it easier to **load data in parallel**. Data could be loaded simultaneously while training. If we don't shuffle the files, we will end up loading the same class of images in a batch, which is <ins>not good for training</ins>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    \"mnist\",\n",
    "    split=[\"train\", \"test\"],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,  # will return tuple (img, label) otherwise dict\n",
    "    with_info=True,  # able to get info about dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = tfds.show_examples(ds_train, ds_info, rows=4, cols=4)\n",
    "print(ds_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_img(image, label):\n",
    "    \"\"\"Normalizes images\"\"\"\n",
    "    return tf.cast(image, tf.float32) / 255.0, label\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "\"\"\"\n",
    "This is what we are going to do similarly when we have our own custom dataset.\n",
    "\"\"\"\n",
    "\n",
    "# Setup for train dataset\n",
    "ds_train = ds_train.map(normalize_img, num_parallel_calls=AUTOTUNE)\n",
    "ds_train = ds_train.cache()\n",
    "ds_train = ds_train.shuffle(ds_info.splits[\"train\"].num_examples)\n",
    "ds_train = ds_train.batch(BATCH_SIZE)\n",
    "# What prefetch does is that while the 128 images (from batch) are being used to train the model,\n",
    "# the dataset will start preparing the next batch of images (prefetching) so that they are ready \n",
    "# for the GPU once it has finished with the current batch. This reduces the time the GPU has to wait\n",
    "# for data to be fed to it.\n",
    "ds_train = ds_train.prefetch(AUTOTUNE)\n",
    "\n",
    "# Setup for test Dataset\n",
    "ds_test = ds_train.map(normalize_img, num_parallel_calls=AUTOTUNE)\n",
    "ds_test = ds_train.batch(128)\n",
    "ds_test = ds_train.prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple model\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input((28, 28, 1)),\n",
    "        layers.Conv2D(32, 3, activation=\"relu\"),\n",
    "        layers.Flatten(),\n",
    "        tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Compiling and training the model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(0.001),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# ds_train is a tf.data.Dataset object, which will return a tuple (x:input_data,y:label) when iterated over.\n",
    "model.fit(ds_train, epochs=5, verbose=2)\n",
    "model.evaluate(ds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Datapipeline for Text Classification\n",
    "\n",
    "We will use imdb dataset for this example. We can do a classification such that a review can be a positive (1) or negative review (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    \"imdb_reviews\",\n",
    "    split=[\"train\", \"test\"],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,  # will return tuple (img, label) otherwise dict\n",
    "    with_info=True,  # able to get info about dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We tokenize the dataset as we cannot send the entire sentence as input to the model.\n",
    "# tokenizer = tfds.features.text.Tokenizer()\n",
    "tokenizer = tfds.features.text.Tokenizer()\n",
    "\n",
    "# Ideally, you wont add all the words in the sentences to the vocabulary. You will only add the words\n",
    "# that occur frequently. But for this example, we will add all the words to the vocabulary.\n",
    "def build_vocabulary():\n",
    "    vocabulary = set()\n",
    "    for text, _ in ds_train:\n",
    "        vocabulary.update(tokenizer.tokenize(text.numpy().lower()))\n",
    "    return vocabulary\n",
    "\n",
    "\n",
    "vocabulary = build_vocabulary()\n",
    "\n",
    "# We encode the tokens to integers so that we can send them to the model.\n",
    "encoder = tfds.features.text.TokenTextEncoder(\n",
    "    list(vocabulary), oov_token=\"<UNK>\", lowercase=True, tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "my_enc is a function that takes in a tensor and a label and returns the encoded tensor and the label.\n",
    "\n",
    ":param text_tensor: A tensor containing the text to be encoded.\n",
    ":param label: A tensor containing the label.\n",
    "\"\"\"\n",
    "def my_enc(text_tensor, label):\n",
    "    encoded_text = encoder.encode(text_tensor.numpy())\n",
    "    return encoded_text, label\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "encode_map_fn is a function that takes in a tensor and a label and returns the encoded tensor and the label.\n",
    "the difference between my_enc and encode_map_fn is that encode_map_fn is a tensorflow function and can be used\n",
    "in the dataset pipeline.\n",
    "\"\"\"\n",
    "def encode_map_fn(text, label):\n",
    "    # py_func doesn't set the shape of the returned tensors.\n",
    "    encoded_text, label = tf.py_function(\n",
    "        my_enc, inp=[text, label], Tout=(tf.int64, tf.int64)\n",
    "    )\n",
    "\n",
    "    # `tf.data.Datasets` work best if all components have a shape set\n",
    "    #  so set the shapes manually:\n",
    "    encoded_text.set_shape([None])\n",
    "    label.set_shape([])\n",
    "\n",
    "    return encoded_text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "ds_train = ds_train.map(encode_map_fn, num_parallel_calls=AUTOTUNE)\n",
    "ds_train = ds_train.cache()\n",
    "ds_train = ds_train.shuffle(1000)\n",
    "# Padding is done so that all the sentences are of the same length, pad to the longest sentence.\n",
    "ds_train = ds_train.padded_batch(32, padded_shapes=([None], ()))\n",
    "ds_train = ds_train.prefetch(AUTOTUNE)\n",
    "\n",
    "ds_test = ds_test.map(encode_map_fn)\n",
    "ds_test = ds_test.padded_batch(32, padded_shapes=([None], ()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model\n",
    "# Normally, we would use an RNN for text classification, but for this example, we will use a simple model.\n",
    "# LSTM is a type of RNN.\n",
    "model = keras.Sequential(\n",
    "    [   \n",
    "        # Masking is done so that the model does not take the padding into account.\n",
    "        # indices 0 is used to pad sequences.\n",
    "        layers.Masking(mask_value=0),\n",
    "        # Embedding layer converts the tokens to vectors.\n",
    "        layers.Embedding(input_dim=len(vocabulary) + 2, output_dim=32),\n",
    "        layers.GlobalAveragePooling1D(),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Compiling and training the model\n",
    "model.compile(\n",
    "    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.Adam(3e-4, clipnorm=1),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model.fit(ds_train, epochs=15, verbose=2)\n",
    "model.evaluate(ds_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
