{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network with Sequential and Functional API\n",
    "\n",
    "## 1. Imports and Configurations\n",
    "\n",
    "#### Notice how in *neuralnetwork.ipynb* when inspecting GPU by nvtop or nvidia-smi, the GPU is not fully utilized. This is because TensorFlow is not configured to use the GPU efficiently.\n",
    "\n",
    "- You need to configure tensorflow to use the GPU more efficiently. \n",
    "- You need to do this before initializing TensorFlow sessions, as it can't be changed after that in the middle of the program.\n",
    "- Use `tf.config.experimental.set_memory_growth` to enable dynamic memory allocation for the GPU. Configuring this will Memory growth setting tells TensorFlow to allocate GPU memory incrementally as needed, rather than allocating the full memory of the GPU upfront. This can be helpful in avoiding the TensorFlow process occupying all of the GPU memory, which might be needed/in use by other processes or applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\"\"\"\n",
    "This code is used to set up TensorFlow to work with the first available GPU on the system \n",
    "and to configure it to use memory more efficiently.\n",
    "\"\"\"\n",
    "# List all the GPUs available, if any. If you have only CPU, this will return an empty list.\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "# If you have more than one GPU, you can select which one you want to use.\n",
    "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "# This line enables memory growth for the first GPU device.\n",
    "config = tf.config.experimental.set_memory_growth(physical_devices[0], True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "Import CIFAR-10 dataset from Keras. CIFAR-10 is a dataset of 50,000 32x32 color training images, labeled over 10 categories, and 10,000 test images.\n",
    "\n",
    "Notice that as we are using a convolutional neural network, we do not need to flatten the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR-10 dataset is located at: /home/jay/.keras/datasets/cifar-10-batches-py\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import get_file\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Get the location of the dataset\n",
    "cifar10_dataset_path = get_file(\n",
    "    fname=\"cifar-10-batches-py\",\n",
    "    origin='https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "    untar=True\n",
    ")\n",
    "\n",
    "print(\"CIFAR-10 dataset is located at:\", cifar10_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Model\n",
    "\n",
    "- `keras.Input(shape=(32, 32, 3))` creates a tensor of shape (32, 32, 3) which is the shape of CIFAR-10 images. 32x32 is the size of the image, and 3 is the number of channels (RGB).\n",
    "\n",
    "- `layerslayers.Conv2D(32, 3, padding=\"valid\", activation=\"relu\")` creates a convolutional layer with 32 filters of size 3x3 kernel size. instead of 3 we can also use (3, 3). padding=\"valid\" means that the output of the convolutional layer will be smaller than the input. padding=\"same\" means that the output of the convolutional layer will be the same size as the input. activation=\"relu\" means that the activation function of the convolutional layer will be ReLU which stands for Rectified Linear Unit and is defined as f(x)=max(0,x).\n",
    "\n",
    "- `layers.MaxPooling2D(pool_size=(2, 2))` creates a pooling layer with a 2x2 pooling window, so half the size of the input in each dimension. This is used to reduce the size of the input and to make the model more robust to changes in the position of the features in the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 30, 30, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 13, 13, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                131136    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 225,034\n",
      "Trainable params: 225,034\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Sequential API (Very convenient, not very flexible)\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(32, 32, 3)),\n",
    "        layers.Conv2D(32, 3, padding=\"valid\", activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, 3, activation=\"relu\"),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(128, 3, activation=\"relu\"),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(10),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# On printing model.summary(), notice that this model has 225K parameters which is pretty small. \n",
    "# The first CNN that revolutionized computer vision, AlexNet, had 60M parameters.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Functional API to create the model and using Batch Normalization. \n",
    "\n",
    "- `keras.Model(inputs=inputs, outputs=outputs)` creates a model with the given inputs and outputs. This is the functional API of Keras. The functional API is used to create models that are more complex than the sequential API allows, such as models with multiple inputs, multiple outputs, shared layers, and residual connections.\n",
    "\n",
    "- `keras.layers.Flatten()` flattens the input. This is used to convert the input from a 2D image to a 1D vector. This is needed as the first layer of the model needs to be a 1D layer.\n",
    "\n",
    "#### Batch Normalization\n",
    "- It involves normalizing the inputs of each layer in such a way that they have a mean of zero and a standard deviation of one. This is similar to how input data is often normalized, but batch normalization does this for the inputs to each layer within the model. This has the effect of **stabilizing the learning process and dramatically reducing the number of training epochs required to train deep networks.**\n",
    "- Batch Normalization is used to normalize the input layer by adjusting and scaling the activations. This is done to speed up the training process and to reduce the sensitivity to network initialization. Batch Normalization is **used after the convolutional layer and before the activation function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model():\n",
    "    inputs = keras.Input(shape=(32, 32, 3))\n",
    "    x = layers.Conv2D(32, 3)(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = keras.activations.relu(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Conv2D(64, 3)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = keras.activations.relu(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Conv2D(128, 3)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = keras.activations.relu(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    outputs = layers.Dense(10)(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "model = my_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.Adam(lr=3e-4),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train and Evaluate Model\n",
    "\n",
    "- When we use the model defined by the functional API, we notice faster training and better accuracy on training due to the use of Batch Normalization.\n",
    "- But, due to overfitting our Test accuracy is lower than the Sequential model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 - 24s - loss: 1.3364 - accuracy: 0.5221\n",
      "Epoch 2/10\n",
      "782/782 - 4s - loss: 0.9492 - accuracy: 0.6687\n",
      "Epoch 3/10\n",
      "782/782 - 4s - loss: 0.7976 - accuracy: 0.7232\n",
      "Epoch 4/10\n",
      "782/782 - 4s - loss: 0.7003 - accuracy: 0.7564\n",
      "Epoch 5/10\n",
      "782/782 - 4s - loss: 0.6159 - accuracy: 0.7875\n",
      "Epoch 6/10\n",
      "782/782 - 4s - loss: 0.5489 - accuracy: 0.8097\n",
      "Epoch 7/10\n",
      "782/782 - 4s - loss: 0.4917 - accuracy: 0.8311\n",
      "Epoch 8/10\n",
      "782/782 - 4s - loss: 0.4320 - accuracy: 0.8543\n",
      "Epoch 9/10\n",
      "782/782 - 4s - loss: 0.3853 - accuracy: 0.8689\n",
      "Epoch 10/10\n",
      "782/782 - 4s - loss: 0.3358 - accuracy: 0.8870\n",
      "\n",
      "Evaluating model...\n",
      "Test loss: 1.0047\n",
      "Test accuracy: 0.7000\n"
     ]
    }
   ],
   "source": [
    "print(\"Training model...\")\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=10, verbose=2)\n",
    "\n",
    "print(\"\\nEvaluating model...\")\n",
    "results = model.evaluate(x_test, y_test, batch_size=64, verbose=0)\n",
    "print(f\"Test loss: {results[0]:.4f}\")\n",
    "print(f\"Test accuracy: {results[1]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
